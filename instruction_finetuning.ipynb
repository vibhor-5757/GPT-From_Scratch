{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2XPlr4sst3S+fLUEhY4PL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JAk-onzNbv6X"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["import os\n","import random\n","\n","import torch"],"metadata":{"id":"viGdC_iub7_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datapath ="],"metadata":{"id":"cgXdBIkLnBLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# creating GPT-2 architechture"],"metadata":{"id":"72NzX7svnLgL"}},{"cell_type":"code","source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of transformer layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}"],"metadata":{"id":"A7xtGvyunRf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LayerNorm(torch.nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n","        self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift"],"metadata":{"id":"WQ-bn4uInTkG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GELU(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))"],"metadata":{"id":"zQvXP_qancKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(torch.nn.Module):\n","    def __init__(self, config): ## config is a dictionary that contains the metadata of the gpt2 model\n","        super().__init__()\n","        self.layers = torch.nn.Sequential(\n","            torch.nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n","            GELU(),\n","            torch.nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"])\n","        )\n","\n","    def forward(self, x):\n","      return self.layers(x)"],"metadata":{"id":"ZmbfyI69nWUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(torch.nn.Module):\n","    def __init__(self, embedding_dim, context_length, num_heads,final_out_dim, dropout, qkv_bias=False):\n","        super().__init__()\n","        assert (final_out_dim % num_heads == 0), \\\n","            \"final_out_dim must be divisible by num_heads\"\n","\n","        self.final_out_dim =final_out_dim\n","        self.num_heads = num_heads\n","        self.head_dim =final_out_dim // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = torch.nn.Linear(embedding_dim,final_out_dim, bias=qkv_bias)\n","        self.W_key = torch.nn.Linear(embedding_dim,final_out_dim, bias=qkv_bias)\n","        self.W_value = torch.nn.Linear(embedding_dim,final_out_dim, bias=qkv_bias)\n","\n","        self.out_proj = torch.nn.Linear(final_out_dim,final_out_dim)  # Linear layer to combine head outputs\n","\n","        self.dropout = torch.nn.Dropout(dropout)\n","\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, embedding_dim = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens,final_out_dim)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens,final_out_dim) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        # attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n","\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.final_out_dim)\n","\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"Yo4RA_FrnX64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(torch.nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            embedding_dim = cfg[\"emb_dim\"],\n","            final_out_dim = cfg[\"emb_dim\"],\n","            context_length = cfg[\"context_length\"],\n","            num_heads = cfg[\"n_heads\"],\n","            dropout = cfg[\"drop_rate\"],\n","            qkv_bias = cfg[\"qkv_bias\"])\n","\n","        self.ff = FeedForward(cfg)\n","\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","\n","        self.drop_shortcut = torch.nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        residual_conn1 = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + residual_conn1  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        residual_conn_2 = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        # 2*4*768\n","        x = self.drop_shortcut(x)\n","        x = x + residual_conn_2  # Add the original input back\n","\n","        return x\n","        # 2*4*768"],"metadata":{"id":"OSUgABlJnaPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GPT(torch.nn.Module):\n","  def __init__(self,cfg):\n","    super().__init__()\n","    self.token_emb = torch.nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","    self.position_emb = torch.nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","\n","    self.drop_emb = torch.nn.Dropout(cfg[\"drop_rate\"])\n","\n","    self.blocks = torch.nn.Sequential(\n","        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n","    )\n","\n","    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","\n","    self.out_head = torch.nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n","\n","  def forward(self, x):\n","    batch_size, seq_len = x.shape\n","    token_emb = self.token_emb(x)\n","    pos_emb = self.position_emb(torch.arange(seq_len, device=x.device))\n","\n","    x = token_emb + pos_emb\n","    x = self.drop_emb(x)\n","\n","    x = self.blocks(x)\n","    x = self.final_norm(x)\n","\n","    logits = self.out_head(x)\n","\n","    return logits"],"metadata":{"id":"5L6cb91rnOpB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# loading the pretrained GPT weights"],"metadata":{"id":"niueyV4nnfeK"}},{"cell_type":"code","source":[],"metadata":{"id":"kR52SLQ4niHl"},"execution_count":null,"outputs":[]}]}